{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Datetime recognizer</h1>\n",
    "Train a model to recognize any datetimes in a given text string. \n",
    "\n",
    "Start with simple iso8601s and then progress to locales, timezones, countries & conventions.\n",
    "\n",
    "\n",
    "Starting from the Keras autoencoder example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''An implementation of sequence to sequence learning to decode localized datetimes from strings\n",
    "Input: \"Monday 31 December 2007 07:07:07.0000000 +02:00 CET/Europe\"\n",
    "or\n",
    "       \"Monday, 24 October 1977 at 09:10:55 Greenwich Mean Time\"\n",
    "Output: \"2007-12-31T07:07:07.00000+02:00\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "\n",
    "\n",
    "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "\n",
    "'''\n",
    "from keras.models import Sequential\n",
    "from keras.engine.training import slice_X\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Nadam\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "#from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import pytz\n",
    "from datetime import datetime, timezone\n",
    "import itertools\n",
    "import random\n",
    "from babel import Locale, localedata, UnknownLocaleError\n",
    "from babel.dates import format_datetime\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    '''\n",
    "    Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    '''\n",
    "    def __init__(self, country_codes, maxlen):\n",
    "        self.chars = set(string.ascii_letters + \n",
    "                         string.punctuation + \n",
    "                         string.digits\n",
    "                        )\n",
    "\n",
    "        ts_from = int(datetime(1980, 1, 1, tzinfo=timezone.utc).timestamp())\n",
    "        ts_to   = int(datetime(1981, 1, 1, tzinfo=timezone.utc).timestamp())\n",
    "        tstamps = [t for t in range(ts_from, ts_to, 60*1000)]\n",
    "        retries = 0\n",
    "        \n",
    "        #Hoover up all the non-ascii characters that might be needed\n",
    "        for country_code in tqdm(country_codes):\n",
    "            try:                            \n",
    "                if pytz.country_timezones.get(country_code):\n",
    "                    for ctzone in pytz.country_timezones(country_code):\n",
    "                        tzone = pytz.timezone(ctzone)\n",
    "                        country_locale = Locale.parse('und_' + country_code)\n",
    "                        for fmt  in ['short',' medium','long','full']:\n",
    "                            ydts = [datetime.utcfromtimestamp(t).replace(tzinfo=tzone) for t in tstamps]\n",
    "                            if random.choice([True, False]):\n",
    "                                X = [format_datetime(dt, format=fmt, locale=country_locale) for dt in ydts]\n",
    "                            else:\n",
    "                                sep = random.choice([' ','T'])\n",
    "                                X = [dt.isoformat(sep) for dt in ydts]\n",
    "\n",
    "                            # So that's a years worth of datetimes for all the countrys & locales & timezones \n",
    "                            for ln in X:\n",
    "                                # Going with case sensitive because some dt formats have an \"i\"\n",
    "                                self.chars = self.chars.union( set(iter(ln)))\n",
    "                                \n",
    "            except UnknownLocaleError:                    \n",
    "                if retries >=21:\n",
    "                    raise # Somethings gone wrong!\n",
    "\n",
    "        self.chars = sorted(self.chars)\n",
    "\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.chars)))\n",
    "        for i, c in enumerate(C[:maxlen]):\n",
    "            X[i, self.char_indices[c]] = 1\n",
    "        return X\n",
    "\n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ctable = CharacterTable(COUNTRY_CODES, MAXLEN_X)\n",
    "#ctable.chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DateTimeDataGenerator(sample_size):   \n",
    "    while True:        \n",
    "        # 0xFFFFFFFF 32 bit unsigned max 2106\n",
    "        # 0x7FFFFFFF 32 bit   signed max 2038\n",
    "        tstamps = [random.randrange(0xFFFFFFFF) for _ in range(sample_size)]\n",
    "\n",
    "        yield tstamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DateTimeIsoGenerator(country_codes, sample_size, maxlen_x, maxlen_y, invert):\n",
    "    \"\"\"Simplest case for training, straing 86012, no timezone.\n",
    "    \"\"\"\n",
    "    dtdg = DateTimeDataGenerator(sample_size)\n",
    "    \n",
    "    while True:        \n",
    "        # 0xFFFFFFFF 32 bit unsigned max 2106\n",
    "        # 0x7FFFFFFF 32 bit   signed max 2038\n",
    "        tstamps = next(dtdg)\n",
    "\n",
    "        ydts = [datetime.utcfromtimestamp(t).isoformat('T') for t in tstamps]\n",
    "        sep = random.choice([' ','T'])\n",
    "        X = [datetime.utcfromtimestamp(t).isoformat(sep) for t in tstamps] \n",
    "\n",
    "        # pad or chop to length\n",
    "        X    = [(pp + (' '*maxlen_x))[:maxlen_x] for pp in X]\n",
    "        ydts = [(pp + (' '*maxlen_y))[:maxlen_y] for pp in ydts]\n",
    "        \n",
    "        if invert:\n",
    "            X = [xx[::-1] for xx in X]\n",
    "            \n",
    "        yield (X, ydts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DateTimeFormattedGenerator(country_codes, sample_size, maxlen_x, maxlen_y, invert):\n",
    "    dtdg = DateTimeDataGenerator(sample_size)\n",
    "    all_locales = localedata.locale_identifiers()\n",
    "    \n",
    "    while True:        \n",
    "        # 0xFFFFFFFF 32 bit unsigned max 2106\n",
    "        # 0x7FFFFFFF 32 bit   signed max 2038\n",
    "        tstamps = next(dtdg)\n",
    "\n",
    "        # Pick a country and timezone\n",
    "        for retries in range(32):\n",
    "            try:\n",
    "                country_code = random.choice(country_codes)\n",
    "                timezone = pytz.timezone(random.choice(pytz.country_timezones(country_code)))\n",
    "                country_locale = Locale.parse('und_' + country_code)\n",
    "                break\n",
    "            except UnknownLocaleError:\n",
    "                print('UnknownLocaleError: %s'%'und_' + country_code, file=sys.stderr)\n",
    "                if retries >=31:\n",
    "                    raise # Somethings gone wrong!\n",
    "                    \n",
    "        ydts = [datetime.utcfromtimestamp(t).replace(tzinfo=timezone).isoformat('T') for t in tstamps]\n",
    "        \n",
    "\n",
    "        fmt = random.choice(['short',' medium','long','full'])\n",
    "        X = [format_datetime(datetime.utcfromtimestamp(t).replace(tzinfo=timezone), \n",
    "                             format=fmt, \n",
    "                             locale=country_locale\n",
    "                            ) for t in tstamps]\n",
    "\n",
    "        # pad or chop to length\n",
    "        X    = [(pp + (' '*maxlen_x))[:maxlen_x] for pp in X]\n",
    "        ydts = [(pp + (' '*maxlen_y))[:maxlen_y] for pp in ydts]\n",
    "        \n",
    "        if invert:\n",
    "            X = [xx[::-1] for xx in X]\n",
    "            \n",
    "        yield (X, ydts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DateTimeVectorGenerator(dt_generator, ctable):    \n",
    "    X_next, y_next = next(dt_generator)\n",
    "    while True:\n",
    "        # X_next are all the same MAXLEN_X length. Same for y_next\n",
    "        X = np.zeros((len(X_next), len(X_next[0]), len(ctable.chars)), dtype=np.bool)\n",
    "        y = np.zeros((len(y_next), len(y_next[0]), len(ctable.chars)), dtype=np.bool)\n",
    "        \n",
    "        for i, sentence in enumerate(X_next):\n",
    "            X[i] = ctable.encode(sentence, len(X_next[0]))\n",
    "            \n",
    "        for i, sentence in enumerate(y_next):\n",
    "            y[i] = ctable.encode(sentence, len(y_next[0]))\n",
    "        \n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_history(learning_hist, data_filename=None):    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    discard = int(math.ceil(len(learning_hist)/100))*5\n",
    "    # skip the first few, they destroy plot scale\n",
    "    plt.plot([i.history['loss']     for i in learning_hist][discard:], label='loss')\n",
    "    plt.plot([i.history['val_loss'] for i in learning_hist][discard:], label='val_loss')\n",
    "\n",
    "    plt.ylabel('error')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.legend()\n",
    "    #plt.ylim([0, 0.005])\n",
    "    plt.title('training error')\n",
    "    if (data_filename is not None):\n",
    "        plt.savefig(data_filename)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.035\n",
    "    drop = 0.75\n",
    "    epochs_drop = 15.0        \n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))    \n",
    "    \n",
    "    if not epoch % epochs_drop:\n",
    "        print('Learning rate adjusted: {}'.format(lrate), file=sys.stderr)\n",
    "    \n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset\n",
    "TRAINING_SIZE   = 1111 #gn1 #50000\n",
    "VALIDATION_SIZE = 111\n",
    "INVERT = True\n",
    "# Try replacing GRU, or SimpleRNN\n",
    "RNN = recurrent.SimpleRNN\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LAYERS = 5\n",
    "MAXLEN_X = 72 # longer than anything expected like 'Monday 31 December 2000 07:07:07.0000000 +02:00 CET/Europe'\n",
    "MAXLEN_Y = len(datetime.now().isoformat(' '))\n",
    "COUNTRY_CODES = ['GB', ] #'US','DE','FR','ES','IT','CA', 'NO','SE','DK','FI','EE','CZ','PL'] # pytz.country_names.keys()\n",
    "\n",
    "HIDDEN_SIZE = MAXLEN_X * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.76it/s]\n"
     ]
    }
   ],
   "source": [
    "ctable = CharacterTable(COUNTRY_CODES, MAXLEN_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1111, 72, 95)\n",
      "y_train.shape: (1111, 26, 95)\n"
     ]
    }
   ],
   "source": [
    "gen_dt = DateTimeIsoGenerator(COUNTRY_CODES, TRAINING_SIZE, MAXLEN_X, MAXLEN_Y, INVERT)\n",
    "#gen_dt = DateTimeFormattedGenerator(COUNTRY_CODES, TRAINING_SIZE, MAXLEN_X, MAXLEN_Y, INVERT)\n",
    "\n",
    "gen_vec_train      = DateTimeVectorGenerator(gen_dt, ctable)\n",
    "gen_vec_validation = DateTimeVectorGenerator(gen_dt, ctable)\n",
    "\n",
    "X_train, y_train = next(gen_vec_train)\n",
    "print('X_train.shape: %s'%str(X_train.shape))\n",
    "print('y_train.shape: %s'%str(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "simplernn_9 (SimpleRNN)          (None, 648)           482112      simplernn_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 648)           0           simplernn_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "repeatvector_3 (RepeatVector)    (None, 26, 648)       0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_10 (SimpleRNN)         (None, 26, 648)       840456      repeatvector_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_11 (SimpleRNN)         (None, 26, 648)       840456      simplernn_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_12 (SimpleRNN)         (None, 26, 648)       840456      simplernn_11[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_13 (SimpleRNN)         (None, 26, 648)       840456      simplernn_12[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_14 (SimpleRNN)         (None, 26, 648)       840456      simplernn_13[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_3 (TimeDistribute(None, 26, 95)        61655       simplernn_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 26, 95)        0           timedistributed_3[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 4746047\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "# note: in a situation where your input sequences have a variable length,\n",
    "# use inputhttps://scherbatsky.local:8000/notebooks/ml/datetime%20autoencoder-Copy1.ipynb#_shape=(None, nb_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN_X, len(ctable.chars))))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "# For the decoder's input, we repeat the encoded input for each time step\n",
    "model.add(RepeatVector(MAXLEN_Y))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer\n",
    "for _ in range(LAYERS):\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# For each of step of the output sequence, decide which character should be chosen\n",
    "model.add(TimeDistributed(Dense(len(ctable.chars))))\n",
    "model.add(Activation('softmax'))\n",
    "#opt = RMSprop(lr=0.035, decay=0.999)\n",
    "#opt = Nadam(lr=0.0035)\n",
    "#opt = Nadam()\n",
    "#opt = SGD(lr=0.065, decay=0.999)\n",
    "opt = SGD(lr=0) # Using scheduler\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopper = EarlyStopping(patience=25, \n",
    "                        verbose=1, \n",
    "                        mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint('../models/checkpoint.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=False, \n",
    "                             mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_hist = []\n",
    "gen_dt_test = DateTimeIsoGenerator(COUNTRY_CODES, TRAINING_SIZE, MAXLEN_X, MAXLEN_Y, INVERT)\n",
    "#gen_dt_test = DateTimeFormattedGenerator(COUNTRY_CODES, TRAINING_SIZE, MAXLEN_X, MAXLEN_Y, INVERT)\n",
    "\n",
    "gen_vec_test = DateTimeVectorGenerator(gen_dt_test, ctable)\n",
    "\n",
    "X_test, y_test = next(gen_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate, checkpointer, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning rate adjusted: 0.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 2.6484 - acc: 0.3308Epoch 00000: val_loss improved from inf to 1.54611, saving model to ../models/checkpoint.00-1.55.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 2.6330 - acc: 0.3336 - val_loss: 1.5461 - val_acc: 0.6067\n",
      "Epoch 2/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 1.1578 - acc: 0.5850Epoch 00001: val_loss improved from 1.54611 to 0.94813, saving model to ../models/checkpoint.01-0.95.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 1.1543 - acc: 0.5856 - val_loss: 0.9481 - val_acc: 0.6188\n",
      "Epoch 3/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.9192 - acc: 0.6313Epoch 00002: val_loss improved from 0.94813 to 0.92302, saving model to ../models/checkpoint.02-0.92.hdf5\n",
      "69993/69993 [==============================] - 51s - loss: 0.9190 - acc: 0.6314 - val_loss: 0.9230 - val_acc: 0.6209\n",
      "Epoch 4/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.9023 - acc: 0.6358Epoch 00003: val_loss improved from 0.92302 to 0.90953, saving model to ../models/checkpoint.03-0.91.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.9022 - acc: 0.6358 - val_loss: 0.9095 - val_acc: 0.6320\n",
      "Epoch 5/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.8866 - acc: 0.6477Epoch 00004: val_loss improved from 0.90953 to 0.89954, saving model to ../models/checkpoint.04-0.90.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.8865 - acc: 0.6477 - val_loss: 0.8995 - val_acc: 0.6511\n",
      "Epoch 6/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 1.4534 - acc: 0.5766Epoch 00005: val_loss did not improve\n",
      "69993/69993 [==============================] - 51s - loss: 1.4643 - acc: 0.5739 - val_loss: 2.1702 - val_acc: 0.3080\n",
      "Epoch 7/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 1.2735 - acc: 0.5419Epoch 00006: val_loss did not improve\n",
      "69993/69993 [==============================] - 51s - loss: 1.2680 - acc: 0.5433 - val_loss: 0.9368 - val_acc: 0.6171\n",
      "Epoch 8/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.9043 - acc: 0.6376Epoch 00007: val_loss did not improve\n",
      "69993/69993 [==============================] - 51s - loss: 0.9040 - acc: 0.6377 - val_loss: 0.9044 - val_acc: 0.6365\n",
      "Epoch 9/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.8811 - acc: 0.6477Epoch 00008: val_loss improved from 0.89954 to 0.88705, saving model to ../models/checkpoint.08-0.89.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.8810 - acc: 0.6478 - val_loss: 0.8871 - val_acc: 0.6369\n",
      "Epoch 10/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.8654 - acc: 0.6554Epoch 00009: val_loss improved from 0.88705 to 0.87538, saving model to ../models/checkpoint.09-0.88.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.8654 - acc: 0.6553 - val_loss: 0.8754 - val_acc: 0.6421\n",
      "Epoch 11/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.8535 - acc: 0.6614Epoch 00010: val_loss improved from 0.87538 to 0.85786, saving model to ../models/checkpoint.10-0.86.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.8534 - acc: 0.6615 - val_loss: 0.8579 - val_acc: 0.6497\n",
      "Epoch 12/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.8290 - acc: 0.6796Epoch 00011: val_loss improved from 0.85786 to 0.81575, saving model to ../models/checkpoint.11-0.82.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.8285 - acc: 0.6801 - val_loss: 0.8157 - val_acc: 0.6892\n",
      "Epoch 13/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.7911 - acc: 0.7054Epoch 00012: val_loss improved from 0.81575 to 0.75612, saving model to ../models/checkpoint.12-0.76.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.7904 - acc: 0.7059 - val_loss: 0.7561 - val_acc: 0.7207\n",
      "Epoch 14/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.7001 - acc: 0.7606Epoch 00013: val_loss improved from 0.75612 to 0.67383, saving model to ../models/checkpoint.13-0.67.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.6994 - acc: 0.7608 - val_loss: 0.6738 - val_acc: 0.7713\n",
      "Epoch 15/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.5986 - acc: 0.8149Epoch 00014: val_loss improved from 0.67383 to 0.58988, saving model to ../models/checkpoint.14-0.59.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.5986 - acc: 0.8148 - val_loss: 0.5899 - val_acc: 0.8073\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning rate adjusted: 0.026250000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.7265 - acc: 0.7786Epoch 00015: val_loss improved from 0.58988 to 0.57621, saving model to ../models/checkpoint.15-0.58.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.7238 - acc: 0.7795 - val_loss: 0.5762 - val_acc: 0.8285\n",
      "Epoch 17/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.4663 - acc: 0.8714Epoch 00016: val_loss improved from 0.57621 to 0.47141, saving model to ../models/checkpoint.16-0.47.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.4656 - acc: 0.8716 - val_loss: 0.4714 - val_acc: 0.8600\n",
      "Epoch 18/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8970Epoch 00017: val_loss improved from 0.47141 to 0.36081, saving model to ../models/checkpoint.17-0.36.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.3829 - acc: 0.8975 - val_loss: 0.3608 - val_acc: 0.9054\n",
      "Epoch 19/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9285Epoch 00018: val_loss improved from 0.36081 to 0.28843, saving model to ../models/checkpoint.18-0.29.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.2966 - acc: 0.9289 - val_loss: 0.2884 - val_acc: 0.9272\n",
      "Epoch 20/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9597Epoch 00019: val_loss improved from 0.28843 to 0.24220, saving model to ../models/checkpoint.19-0.24.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.2086 - acc: 0.9598 - val_loss: 0.2422 - val_acc: 0.9397\n",
      "Epoch 21/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9790Epoch 00020: val_loss improved from 0.24220 to 0.15499, saving model to ../models/checkpoint.20-0.15.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.1453 - acc: 0.9792 - val_loss: 0.1550 - val_acc: 0.9761\n",
      "Epoch 22/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9948Epoch 00021: val_loss improved from 0.15499 to 0.11148, saving model to ../models/checkpoint.21-0.11.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0874 - acc: 0.9949 - val_loss: 0.1115 - val_acc: 0.9861\n",
      "Epoch 23/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9990Epoch 00022: val_loss improved from 0.11148 to 0.08893, saving model to ../models/checkpoint.22-0.09.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0582 - acc: 0.9990 - val_loss: 0.0889 - val_acc: 0.9893\n",
      "Epoch 24/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 1.0000Epoch 00023: val_loss improved from 0.08893 to 0.07329, saving model to ../models/checkpoint.23-0.07.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0403 - acc: 1.0000 - val_loss: 0.0733 - val_acc: 0.9910\n",
      "Epoch 25/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 1.0000Epoch 00024: val_loss improved from 0.07329 to 0.06253, saving model to ../models/checkpoint.24-0.06.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0312 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 0.9924\n",
      "Epoch 26/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 1.0000Epoch 00025: val_loss improved from 0.06253 to 0.05450, saving model to ../models/checkpoint.25-0.05.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0251 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 0.9934\n",
      "Epoch 27/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 1.0000Epoch 00026: val_loss improved from 0.05450 to 0.04831, saving model to ../models/checkpoint.26-0.05.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0207 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9938\n",
      "Epoch 28/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 1.0000Epoch 00027: val_loss improved from 0.04831 to 0.04341, saving model to ../models/checkpoint.27-0.04.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0176 - acc: 1.0000 - val_loss: 0.0434 - val_acc: 0.9945\n",
      "Epoch 29/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 1.0000Epoch 00028: val_loss improved from 0.04341 to 0.03945, saving model to ../models/checkpoint.28-0.04.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0395 - val_acc: 0.9955\n",
      "Epoch 30/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 1.0000Epoch 00029: val_loss improved from 0.03945 to 0.03694, saving model to ../models/checkpoint.29-0.04.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0369 - val_acc: 0.9955\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning rate adjusted: 0.019687500000000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 1.0000Epoch 00030: val_loss improved from 0.03694 to 0.03474, saving model to ../models/checkpoint.30-0.03.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0347 - val_acc: 0.9958\n",
      "Epoch 32/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 1.0000Epoch 00031: val_loss improved from 0.03474 to 0.03280, saving model to ../models/checkpoint.31-0.03.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0113 - acc: 1.0000 - val_loss: 0.0328 - val_acc: 0.9958\n",
      "Epoch 33/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 1.0000Epoch 00032: val_loss improved from 0.03280 to 0.03109, saving model to ../models/checkpoint.32-0.03.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0311 - val_acc: 0.9962\n",
      "Epoch 34/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 1.0000Epoch 00033: val_loss improved from 0.03109 to 0.02956, saving model to ../models/checkpoint.33-0.03.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0096 - acc: 1.0000 - val_loss: 0.0296 - val_acc: 0.9969\n",
      "Epoch 35/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 1.0000Epoch 00034: val_loss improved from 0.02956 to 0.02818, saving model to ../models/checkpoint.34-0.03.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9972\n",
      "Epoch 36/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 1.0000Epoch 00035: val_loss improved from 0.02818 to 0.02693, saving model to ../models/checkpoint.35-0.03.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0084 - acc: 1.0000 - val_loss: 0.0269 - val_acc: 0.9979\n",
      "Epoch 37/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 1.0000Epoch 00036: val_loss improved from 0.02693 to 0.02580, saving model to ../models/checkpoint.36-0.03.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0258 - val_acc: 0.9979\n",
      "Epoch 38/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 1.0000Epoch 00037: val_loss improved from 0.02580 to 0.02476, saving model to ../models/checkpoint.37-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0248 - val_acc: 0.9983\n",
      "Epoch 39/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 1.0000Epoch 00038: val_loss improved from 0.02476 to 0.02380, saving model to ../models/checkpoint.38-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0238 - val_acc: 0.9983\n",
      "Epoch 40/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 1.0000Epoch 00039: val_loss improved from 0.02380 to 0.02293, saving model to ../models/checkpoint.39-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0229 - val_acc: 0.9986\n",
      "Epoch 41/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 1.0000Epoch 00040: val_loss improved from 0.02293 to 0.02212, saving model to ../models/checkpoint.40-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0221 - val_acc: 0.9986\n",
      "Epoch 42/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 1.0000Epoch 00041: val_loss improved from 0.02212 to 0.02137, saving model to ../models/checkpoint.41-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0214 - val_acc: 0.9986\n",
      "Epoch 43/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 1.0000Epoch 00042: val_loss improved from 0.02137 to 0.02068, saving model to ../models/checkpoint.42-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0207 - val_acc: 0.9986\n",
      "Epoch 44/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 1.0000Epoch 00043: val_loss improved from 0.02068 to 0.02003, saving model to ../models/checkpoint.43-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0200 - val_acc: 0.9986\n",
      "Epoch 45/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 1.0000Epoch 00044: val_loss improved from 0.02003 to 0.01957, saving model to ../models/checkpoint.44-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0196 - val_acc: 0.9986\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning rate adjusted: 0.014765625000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 1.0000Epoch 00045: val_loss improved from 0.01957 to 0.01914, saving model to ../models/checkpoint.45-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0191 - val_acc: 0.9986\n",
      "Epoch 47/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 1.0000Epoch 00046: val_loss improved from 0.01914 to 0.01872, saving model to ../models/checkpoint.46-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0187 - val_acc: 0.9986\n",
      "Epoch 48/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 1.0000Epoch 00047: val_loss improved from 0.01872 to 0.01833, saving model to ../models/checkpoint.47-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0183 - val_acc: 0.9990\n",
      "Epoch 49/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 1.0000Epoch 00048: val_loss improved from 0.01833 to 0.01795, saving model to ../models/checkpoint.48-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0180 - val_acc: 0.9990\n",
      "Epoch 50/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 1.0000Epoch 00049: val_loss improved from 0.01795 to 0.01759, saving model to ../models/checkpoint.49-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0176 - val_acc: 0.9990\n",
      "Epoch 51/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 1.0000Epoch 00050: val_loss improved from 0.01759 to 0.01725, saving model to ../models/checkpoint.50-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0043 - acc: 1.0000 - val_loss: 0.0172 - val_acc: 0.9990\n",
      "Epoch 52/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 1.0000Epoch 00051: val_loss improved from 0.01725 to 0.01691, saving model to ../models/checkpoint.51-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0042 - acc: 1.0000 - val_loss: 0.0169 - val_acc: 0.9990\n",
      "Epoch 53/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 1.0000Epoch 00052: val_loss improved from 0.01691 to 0.01660, saving model to ../models/checkpoint.52-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0166 - val_acc: 0.9990\n",
      "Epoch 54/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 1.0000Epoch 00053: val_loss improved from 0.01660 to 0.01629, saving model to ../models/checkpoint.53-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9990\n",
      "Epoch 55/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 1.0000Epoch 00054: val_loss improved from 0.01629 to 0.01600, saving model to ../models/checkpoint.54-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9990\n",
      "Epoch 56/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 1.0000Epoch 00055: val_loss improved from 0.01600 to 0.01572, saving model to ../models/checkpoint.55-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 0.9990\n",
      "Epoch 57/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 1.0000Epoch 00056: val_loss improved from 0.01572 to 0.01545, saving model to ../models/checkpoint.56-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 0.9990\n",
      "Epoch 58/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 1.0000Epoch 00057: val_loss improved from 0.01545 to 0.01519, saving model to ../models/checkpoint.57-0.02.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0152 - val_acc: 0.9990\n",
      "Epoch 59/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 1.0000Epoch 00058: val_loss improved from 0.01519 to 0.01494, saving model to ../models/checkpoint.58-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0149 - val_acc: 0.9990\n",
      "Epoch 60/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 1.0000Epoch 00059: val_loss improved from 0.01494 to 0.01476, saving model to ../models/checkpoint.59-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9990\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning rate adjusted: 0.011074218750000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 1.0000Epoch 00060: val_loss improved from 0.01476 to 0.01458, saving model to ../models/checkpoint.60-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9990\n",
      "Epoch 62/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 1.0000Epoch 00061: val_loss improved from 0.01458 to 0.01441, saving model to ../models/checkpoint.61-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0144 - val_acc: 0.9990\n",
      "Epoch 63/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 1.0000Epoch 00062: val_loss improved from 0.01441 to 0.01424, saving model to ../models/checkpoint.62-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0142 - val_acc: 0.9990\n",
      "Epoch 64/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 1.0000Epoch 00063: val_loss improved from 0.01424 to 0.01408, saving model to ../models/checkpoint.63-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0141 - val_acc: 0.9990\n",
      "Epoch 65/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 1.0000Epoch 00064: val_loss improved from 0.01408 to 0.01392, saving model to ../models/checkpoint.64-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 0.9990\n",
      "Epoch 66/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 1.0000Epoch 00065: val_loss improved from 0.01392 to 0.01377, saving model to ../models/checkpoint.65-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0138 - val_acc: 0.9990\n",
      "Epoch 67/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 1.0000Epoch 00066: val_loss improved from 0.01377 to 0.01361, saving model to ../models/checkpoint.66-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0136 - val_acc: 0.9990\n",
      "Epoch 68/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 1.0000Epoch 00067: val_loss improved from 0.01361 to 0.01346, saving model to ../models/checkpoint.67-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0135 - val_acc: 0.9990\n",
      "Epoch 69/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 1.0000Epoch 00068: val_loss improved from 0.01346 to 0.01332, saving model to ../models/checkpoint.68-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0133 - val_acc: 0.9990\n",
      "Epoch 70/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 1.0000Epoch 00069: val_loss improved from 0.01332 to 0.01318, saving model to ../models/checkpoint.69-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0132 - val_acc: 0.9990\n",
      "Epoch 71/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 1.0000Epoch 00070: val_loss improved from 0.01318 to 0.01304, saving model to ../models/checkpoint.70-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0130 - val_acc: 0.9990\n",
      "Epoch 72/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 1.0000Epoch 00071: val_loss improved from 0.01304 to 0.01291, saving model to ../models/checkpoint.71-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0129 - val_acc: 0.9990\n",
      "Epoch 73/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 1.0000Epoch 00072: val_loss improved from 0.01291 to 0.01278, saving model to ../models/checkpoint.72-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 0.9990\n",
      "Epoch 74/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 1.0000Epoch 00073: val_loss improved from 0.01278 to 0.01265, saving model to ../models/checkpoint.73-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0126 - val_acc: 0.9990\n",
      "Epoch 75/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 1.0000Epoch 00074: val_loss improved from 0.01265 to 0.01255, saving model to ../models/checkpoint.74-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0126 - val_acc: 0.9990\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning rate adjusted: 0.0083056640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 1.0000Epoch 00075: val_loss improved from 0.01255 to 0.01246, saving model to ../models/checkpoint.75-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0125 - val_acc: 0.9990\n",
      "Epoch 77/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 1.0000Epoch 00076: val_loss improved from 0.01246 to 0.01237, saving model to ../models/checkpoint.76-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0124 - val_acc: 0.9990\n",
      "Epoch 78/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 1.0000Epoch 00077: val_loss improved from 0.01237 to 0.01228, saving model to ../models/checkpoint.77-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0123 - val_acc: 0.9990\n",
      "Epoch 79/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 1.0000Epoch 00078: val_loss improved from 0.01228 to 0.01219, saving model to ../models/checkpoint.78-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0122 - val_acc: 0.9990\n",
      "Epoch 80/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 1.0000Epoch 00079: val_loss improved from 0.01219 to 0.01210, saving model to ../models/checkpoint.79-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0121 - val_acc: 0.9990\n",
      "Epoch 81/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 1.0000Epoch 00080: val_loss improved from 0.01210 to 0.01202, saving model to ../models/checkpoint.80-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0120 - val_acc: 0.9990\n",
      "Epoch 82/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 1.0000Epoch 00081: val_loss improved from 0.01202 to 0.01193, saving model to ../models/checkpoint.81-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0119 - val_acc: 0.9990\n",
      "Epoch 83/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 1.0000Epoch 00082: val_loss improved from 0.01193 to 0.01185, saving model to ../models/checkpoint.82-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0119 - val_acc: 0.9990\n",
      "Epoch 84/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 1.0000Epoch 00083: val_loss improved from 0.01185 to 0.01177, saving model to ../models/checkpoint.83-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0118 - val_acc: 0.9990\n",
      "Epoch 85/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 1.0000Epoch 00084: val_loss improved from 0.01177 to 0.01169, saving model to ../models/checkpoint.84-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 0.9990\n",
      "Epoch 86/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 1.0000Epoch 00085: val_loss improved from 0.01169 to 0.01161, saving model to ../models/checkpoint.85-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0116 - val_acc: 0.9990\n",
      "Epoch 87/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 1.0000Epoch 00086: val_loss improved from 0.01161 to 0.01153, saving model to ../models/checkpoint.86-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0115 - val_acc: 0.9990\n",
      "Epoch 88/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 1.0000Epoch 00087: val_loss improved from 0.01153 to 0.01145, saving model to ../models/checkpoint.87-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0115 - val_acc: 0.9990\n",
      "Epoch 89/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 1.0000Epoch 00088: val_loss improved from 0.01145 to 0.01138, saving model to ../models/checkpoint.88-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0114 - val_acc: 0.9990\n",
      "Epoch 90/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 1.0000Epoch 00089: val_loss improved from 0.01138 to 0.01132, saving model to ../models/checkpoint.89-0.01.hdf5\n",
      "69993/69993 [==============================] - 51s - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0113 - val_acc: 0.9990\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning rate adjusted: 0.006229248046875001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 1.0000Epoch 00090: val_loss improved from 0.01132 to 0.01127, saving model to ../models/checkpoint.90-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0113 - val_acc: 0.9990\n",
      "Epoch 92/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 1.0000Epoch 00091: val_loss improved from 0.01127 to 0.01121, saving model to ../models/checkpoint.91-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0112 - val_acc: 0.9990\n",
      "Epoch 93/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 1.0000Epoch 00092: val_loss improved from 0.01121 to 0.01116, saving model to ../models/checkpoint.92-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0112 - val_acc: 0.9990\n",
      "Epoch 94/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 1.0000Epoch 00093: val_loss improved from 0.01116 to 0.01110, saving model to ../models/checkpoint.93-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0111 - val_acc: 0.9990\n",
      "Epoch 95/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 1.0000Epoch 00094: val_loss improved from 0.01110 to 0.01105, saving model to ../models/checkpoint.94-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0110 - val_acc: 0.9990\n",
      "Epoch 96/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 1.0000Epoch 00095: val_loss improved from 0.01105 to 0.01099, saving model to ../models/checkpoint.95-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0110 - val_acc: 0.9990\n",
      "Epoch 97/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 1.0000Epoch 00096: val_loss improved from 0.01099 to 0.01094, saving model to ../models/checkpoint.96-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0109 - val_acc: 0.9990\n",
      "Epoch 98/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 1.0000Epoch 00097: val_loss improved from 0.01094 to 0.01089, saving model to ../models/checkpoint.97-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0109 - val_acc: 0.9990\n",
      "Epoch 99/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 1.0000Epoch 00098: val_loss improved from 0.01089 to 0.01084, saving model to ../models/checkpoint.98-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0108 - val_acc: 0.9990\n",
      "Epoch 100/100\n",
      "68882/69993 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 1.0000Epoch 00099: val_loss improved from 0.01084 to 0.01079, saving model to ../models/checkpoint.99-0.01.hdf5\n",
      "69993/69993 [==============================] - 52s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0108 - val_acc: 0.9990\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAugAAAGJCAYAAADYGsbyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f999048d470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model each generation and show predictions against the validation dataset\n",
    "\n",
    "for iteration in range(1, 1+1):\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    \n",
    "    # switch locale (probably)\n",
    "    gen_dt_train = DateTimeIsoGenerator(COUNTRY_CODES, TRAINING_SIZE, MAXLEN_X, MAXLEN_Y, INVERT)  \n",
    "    gen_dt_val   = DateTimeIsoGenerator(COUNTRY_CODES, VALIDATION_SIZE, MAXLEN_X, MAXLEN_Y, INVERT)\n",
    "    ## or\n",
    "    #gen_dt_train = DateTimeFormattedGenerator(COUNTRY_CODES, TRAINING_SIZE, MAXLEN_X, MAXLEN_Y, INVERT)\n",
    "    #gen_dt_val   = DateTimeFormattedGenerator(COUNTRY_CODES, VALIDATION_SIZE, MAXLEN_X, MAXLEN_Y, INVERT)\n",
    "\n",
    "    gen_vec_train      = DateTimeVectorGenerator(gen_dt_train, ctable)\n",
    "    gen_vec_validation = DateTimeVectorGenerator(gen_dt_val,   ctable)\n",
    "    \n",
    "    learning_hist.append(\n",
    "            model.fit_generator( gen_vec_train, \n",
    "                                samples_per_epoch=69993, #88880, #, \n",
    "                                nb_epoch=100, \n",
    "                                verbose=1, \n",
    "                                callbacks=callbacks_list, \n",
    "                                validation_data=gen_vec_validation, \n",
    "                                nb_val_samples=VALIDATION_SIZE, \n",
    "                                #class_weight={}, \n",
    "                                #max_q_size=10, \n",
    "                                #nb_worker=1, \n",
    "                                #pickle_safe=False\n",
    "                               )\n",
    "            )\n",
    "    ###\n",
    "    # Select 3 samples from the test set so we can visualize errors\n",
    "    if iteration % 5 == 0:\n",
    "        model.save('../models/datatime_autoencoder_model.h5')\n",
    "        for i in range(3):\n",
    "            X_test, y_test = next(gen_vec_test)            \n",
    "            preds = model.predict_classes(X_test, verbose=0)\n",
    "            q = ctable.decode(X_test[0])\n",
    "            correct = ctable.decode(y_test[0])\n",
    "            guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "            print('Q', q[::-1] if INVERT else q )\n",
    "            print('T', correct)\n",
    "            print(colors.ok + '☑' + colors.close if correct == guess else colors.fail + '☒' + colors.close, guess)\n",
    "\n",
    "visualize_history(learning_hist)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!! rm ../models/checkpoint.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-4475975b321a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-4475975b321a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Bumped lr=0.05, nb_epoch=1,  576(72*8) Hidden, iterations=50, Relu\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Bumped lr=0.05, nb_epoch=1,  576(72*8) Hidden, iterations=50, Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Results</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--------------------------------------------------\n",
    "Iteration 151\n",
    "Epoch 1/1\n",
    "69993/69993 [==============================] - 35s - loss: 0.7406 - acc: 0.7026 - val_loss: 0.7502 - val_acc: 0.7017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--------------------------------------------------\n",
    "Iteration 125\n",
    "Epoch 1/1\n",
    "69993/69993 [==============================] - 35s - loss: 1.5492 - acc: 0.5519 - val_loss: 1.5425 - val_acc: 0.5606\n",
    "Q 2012-02-14T07:26:58                                                     \n",
    "T 2012-02-14T07:26:58       \n",
    "☒ 2090-01-0000:::::         \n",
    "Q 2012-02-14T07:26:58                                                     \n",
    "T 2012-02-14T07:26:58       \n",
    "☒ 2090-01-0000:::::         \n",
    "Q 2012-02-14T07:26:58                                                     \n",
    "T 2012-02-14T07:26:58       \n",
    "☒ 2090-01-0000:::::   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "------------sgd hidden*5--------------------------------------\n",
    "Iteration 50\n",
    "Epoch 1/1\n",
    "69993/69993 [==============================] - 19s - loss: 1.6622 - acc: 0.5932 - val_loss: 1.6589 - val_acc: 0.5925\n",
    "Q 2042-10-15 00:45:32                                                     \n",
    "T 2042-10-15T00:45:32       \n",
    "☒ 2000-00-01T11::::         \n",
    "Q 2042-10-15 00:45:32                                                     \n",
    "T 2042-10-15T00:45:32       \n",
    "☒ 2000-00-01T11::::         \n",
    "Q 2042-10-15 00:45:32                                                     \n",
    "T 2042-10-15T00:45:32       \n",
    "☒ 2000-00-01T11:::: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Iteration 50\n",
    "Epoch 1/1\n",
    "69993/69993 [==============================] - 57s - loss: 1.3140 - acc: 0.5820 - val_loss: 1.3093 - val_acc: 0.5811\n",
    "Q 2045-04-22T12:16:56                                                     \n",
    "T 2045-04-22T12:16:56       \n",
    "☒ 200--00--1TT:::::::       \n",
    "Q 2045-04-22T12:16:56                                                     \n",
    "T 2045-04-22T12:16:56       \n",
    "☒ 200--00--1TT:::::::       \n",
    "Q 2045-04-22T12:16:56                                                     \n",
    "T 2045-04-22T12:16:56       \n",
    "☒ 200--00--1TT:::::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2 Layer, Relu, lr=0.01, 360(72*5) Hidden, ISO8601</h2>\n",
    "~~~\n",
    "Iteration 60\n",
    "Epoch 1/1\n",
    "69993/69993 [==============================] - 38s - loss: 0.7598 - acc: 0.7147 - val_loss: 0.7792 - val_acc: 0.7100\n",
    "Q 2082-12-26 01:57:13                                                     \n",
    "T 2082-12-26T01:57:13       \n",
    "☒ 2022-12-22T12:22:12       \n",
    "Q 2082-12-26 01:57:13                                                     \n",
    "T 2082-12-26T01:57:13       \n",
    "☒ 2022-12-22T12:22:12       \n",
    "Q 2082-12-26 01:57:13                                                     \n",
    "T 2082-12-26T01:57:13       \n",
    "☒ 2022-12-22T12:22:12\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2 Layer Adam, 333 Hidden, ISO8601</h2>\n",
    "~~~\n",
    "Iteration 20\n",
    "Epoch 1/1\n",
    "69993/69993 [==============================] - 41s - loss: 1.9638 - acc: 0.3964 - val_loss: 1.9701 - val_acc: 0.3957\n",
    "Q 2065-02-19 13:56:52                                                     \n",
    "T 2065-02-19T13:56:52       \n",
    "☒ 200000000000              \n",
    "Q 2065-02-19 13:56:52                                                     \n",
    "T 2065-02-19T13:56:52       \n",
    "☒ 200000000000              \n",
    "Q 2065-02-19 13:56:52                                                     \n",
    "T 2065-02-19T13:56:52       \n",
    "☒ 200000000000 \n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>3 Layer, Adam, 333 hidden, ISO8601</h2>\n",
    "~~~\n",
    "Iteration 200\n",
    "Epoch 1/1\n",
    "46662/46662 [==============================] - 32s - loss: 0.5656 - acc: 0.7843 - val_loss: 0.5589 - val_acc: 0.7904\n",
    "Q 2007-07-14T13:26:06                                                     \n",
    "T 2007-07-14T13:26:06       \n",
    "☒ 2007-07-11T11:57:57       \n",
    "Q 2007-07-14T13:26:06                                                     \n",
    "T 2007-07-14T13:26:06       \n",
    "☒ 2007-07-11T11:57:57       \n",
    "Q 2007-07-14T13:26:06                                                     \n",
    "T 2007-07-14T13:26:06       \n",
    "☒ 2007-07-11T11:57:57     \n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 2035-06-25T19:58:17                                                     \n",
      "T 2035-06-25T19:58:17       \n",
      "\u001b[92m☑\u001b[0m 2035-06-25T19:58:17       \n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = next(gen_vec_test)            \n",
    "preds = model.predict_classes(X_test, verbose=0)\n",
    "q = ctable.decode(X_test[0])\n",
    "correct = ctable.decode(y_test[0])\n",
    "guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "print('Q', q[::-1] if INVERT else q )\n",
    "print('T', correct)\n",
    "print(colors.ok + '☑' + colors.close if correct == guess else colors.fail + '☒' + colors.close, guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(learning_hist[0].history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtsAAAGJCAYAAABb3v/JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f997743eba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "discard = int(math.ceil(len(learning_hist)/100))*20\n",
    "# skip the first few, they destroy plot scale\n",
    "plt.plot([i for i in learning_hist[-1].history['loss'][discard:]], label='loss')\n",
    "plt.plot([i for i in learning_hist[-1].history['val_loss'][discard:]], label='val_loss')\n",
    "\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend()\n",
    "#plt.ylim([0, 0.005])\n",
    "plt.title('training error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(learning_hist[0].history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../models/datatime_autoencoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
